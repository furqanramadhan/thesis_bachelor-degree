{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot settings\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (12, 5)\n",
    "rcParams['xtick.labelsize'] = 12\n",
    "rcParams['ytick.labelsize'] = 12\n",
    "rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "output_dir = './test_planting'\n",
    "csv_output_dir = './test_planting/csv'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(csv_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plt(filename):\n",
    "    \"\"\"Save the current plot to the output directory.\"\"\"\n",
    "    plt.savefig(f'{output_dir}/{filename}.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING AND PREPROCESSING DATA\n",
      "==================================================\n",
      "Data loaded. Shape: (7693, 13)\n",
      "Date range: 2005-01-01 00:00:00 to 2025-03-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# ==============================\n",
    "print(\"=\"*50)\n",
    "print(\"LOADING AND PREPROCESSING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load BMKG data\n",
    "bmkg_data = pd.read_csv('/run/media/cryptedlm/localdisk/Kuliah/Tugas Akhir/Dataset/Data BMKG/Stasiun Klimatologi Aceh/CSV/BMKG_Data_All.csv', index_col=0, parse_dates=True)\n",
    "print(f\"Data loaded. Shape: {bmkg_data.shape}\")\n",
    "print(f\"Date range: {bmkg_data.index.min()} to {bmkg_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types before conversion:\n",
      "Year         int64\n",
      "Month        int64\n",
      "Day          int64\n",
      "TN         float64\n",
      "TX         float64\n",
      "TAVG       float64\n",
      "RH_AVG     float64\n",
      "RR         float64\n",
      "SS         float64\n",
      "FF_X       float64\n",
      "DDD_X      float64\n",
      "FF_AVG     float64\n",
      "DDD_CAR     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(\"\\nData types before conversion:\")\n",
    "for col in bmkg_data.columns:\n",
    "    if bmkg_data[col].dtype != 'object':  # Only modify numeric columns\n",
    "        bmkg_data[col] = bmkg_data[col].replace([8888, 9999], np.nan)\n",
    "print(bmkg_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean data: convert cols to numeric, replace special values with NaN\n",
    "# cols_to_fix = ['TN', 'TX', 'TAVG', 'RH_AVG', 'RR', 'SS', 'FF_X', 'DDD_X', 'FF_AVG']\n",
    "# for col in cols_to_fix:\n",
    "#     bmkg_data[col] = pd.to_numeric(bmkg_data[col], errors='coerce')\n",
    "\n",
    "# # Handle special missing value codes\n",
    "# for col in bmkg_data.columns:\n",
    "#     if bmkg_data[col].dtype != 'object':  # Only modify numeric columns\n",
    "#         bmkg_data[col] = bmkg_data[col].replace([8888, 9999], np.nan)\n",
    "\n",
    "# print(\"\\nData types after conversion:\")\n",
    "# print(bmkg_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values percentage by column:\n",
      "Year        0.000000\n",
      "Month       0.000000\n",
      "Day         0.000000\n",
      "TN          1.052905\n",
      "TX          0.779930\n",
      "TAVG        0.103991\n",
      "RH_AVG      0.025998\n",
      "RR         36.747693\n",
      "SS          0.740933\n",
      "FF_X        1.390875\n",
      "DDD_X       4.952554\n",
      "FF_AVG      7.357338\n",
      "DDD_CAR     1.962823\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "missing_percentage = bmkg_data.isna().mean() * 100\n",
    "print(\"\\nMissing values percentage by column:\")\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "HANDLING MISSING VALUES WITH STL DECOMPOSITION\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 2. ADVANCED MISSING VALUE HANDLING WITH STL DECOMPOSITION\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HANDLING MISSING VALUES WITH STL DECOMPOSITION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Focus on our target variables for rice cultivation\n",
    "target_vars = ['RR', 'TAVG', 'RH_AVG']\n",
    "bmkg_for_forecast = bmkg_data[target_vars].copy()\n",
    "\n",
    "# First apply simple forward/backward fill for initial handling\n",
    "bmkg_filled = bmkg_for_forecast.fillna(method='ffill').fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stl_imputation(series, seasonal_period):\n",
    "    \"\"\"\n",
    "    Use STL decomposition to impute missing values in a time series.\n",
    "    Enhanced version with better error handling and explicit period setting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas Series\n",
    "        Time series with NaN values to impute\n",
    "    seasonal_period : int\n",
    "        Seasonal period for STL decomposition\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas Series\n",
    "        Series with imputed values\n",
    "    \"\"\"\n",
    "    # Create a copy of the series for manipulation\n",
    "    imputed_series = series.copy()\n",
    "    \n",
    "    # Get indices of missing values\n",
    "    missing_indices = series[series.isna()].index\n",
    "    \n",
    "    # If no missing values, return the original series\n",
    "    if len(missing_indices) == 0:\n",
    "        return series\n",
    "    \n",
    "    # Check if we have enough data for STL\n",
    "    if len(series) < seasonal_period * 2:\n",
    "        print(f\"  Not enough data for STL (need {seasonal_period * 2}, have {len(series)})\")\n",
    "        return series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "    \n",
    "    # If too many missing values, use simpler method\n",
    "    if series.isna().mean() > 0.3:  # If more than 30% missing\n",
    "        print(f\"  Too many missing values ({series.isna().mean()*100:.1f}%). Using simple imputation.\")\n",
    "        return series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "    \n",
    "    # Fill missing values with a simple method for initial STL\n",
    "    temp_filled = series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "    \n",
    "    try:\n",
    "        # Apply STL decomposition with explicit period setting\n",
    "        stl = STL(temp_filled, \n",
    "                  seasonal=seasonal_period,\n",
    "                  period=seasonal_period,  # Explicitly set the period\n",
    "                  robust=True)\n",
    "        result = stl.fit()\n",
    "        \n",
    "        # Extract components\n",
    "        trend = result.trend\n",
    "        seasonal = result.seasonal\n",
    "        residual = result.resid\n",
    "        \n",
    "        # Impute missing values using the components\n",
    "        for idx in missing_indices:\n",
    "            if idx in trend.index:\n",
    "                # Reconstruct the value using trend and seasonal component\n",
    "                imputed_series[idx] = trend[idx] + seasonal[idx]\n",
    "        \n",
    "        # For any remaining NaN (e.g., at the edges), use original simple imputation\n",
    "        imputed_series = imputed_series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "        \n",
    "        return imputed_series\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  STL imputation failed: {str(e)}. Using simple imputation.\")\n",
    "        # If STL fails, try a simpler seasonal decomposition\n",
    "        try:\n",
    "            from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "            \n",
    "            # Use seasonal_decompose which is more forgiving with data requirements\n",
    "            result = seasonal_decompose(temp_filled, \n",
    "                                       period=seasonal_period,\n",
    "                                       model='additive',\n",
    "                                       extrapolate_trend='freq')\n",
    "            \n",
    "            # Extract components\n",
    "            trend = result.trend\n",
    "            seasonal = result.seasonal\n",
    "            \n",
    "            # Impute missing values\n",
    "            for idx in missing_indices:\n",
    "                if idx in trend.index and not np.isnan(trend[idx]) and not np.isnan(seasonal[idx]):\n",
    "                    imputed_series[idx] = trend[idx] + seasonal[idx]\n",
    "            \n",
    "            # For any remaining NaN, use original simple imputation\n",
    "            imputed_series = imputed_series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "            \n",
    "            print(f\"  Used seasonal_decompose as fallback\")\n",
    "            return imputed_series\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"  Seasonal decomposition also failed: {str(e2)}. Using simple imputation.\")\n",
    "            return temp_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stl_imputation(series, seasonal_period):\n",
    "    \"\"\"Modified STL imputation with better error handling\"\"\"\n",
    "    # Create a copy of the series for manipulation\n",
    "    imputed_series = series.copy()\n",
    "    \n",
    "    # Get indices of missing values\n",
    "    missing_indices = series[series.isna()].index\n",
    "    \n",
    "    # If no missing values, return the original series\n",
    "    if len(missing_indices) == 0:\n",
    "        return series\n",
    "    \n",
    "    # Check if we have enough data for STL\n",
    "    if len(series) < seasonal_period * 2:\n",
    "        print(f\"  Not enough data for STL (need {seasonal_period * 2}, have {len(series)})\")\n",
    "        return series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "    \n",
    "    # If too many missing values, use simpler method\n",
    "    if series.isna().mean() > 0.3:  # If more than 30% missing\n",
    "        print(f\"  Too many missing values ({series.isna().mean()*100:.1f}%). Using simple imputation.\")\n",
    "        return series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "    \n",
    "    # Fill missing values with a simple method for initial STL\n",
    "    temp_filled = series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "    \n",
    "    try:\n",
    "        # Apply STL decomposition with enforced periodicity\n",
    "        stl = STL(temp_filled, \n",
    "                 seasonal=seasonal_period, \n",
    "                 robust=True,\n",
    "                 period=seasonal_period)  # Explicitly set period\n",
    "        result = stl.fit()\n",
    "        \n",
    "        # Extract components\n",
    "        trend = result.trend\n",
    "        seasonal = result.seasonal\n",
    "        \n",
    "        # Impute missing values using the components\n",
    "        for idx in missing_indices:\n",
    "            if idx in trend.index:\n",
    "                # Reconstruct the value using trend and seasonal component\n",
    "                imputed_series[idx] = trend[idx] + seasonal[idx]\n",
    "        \n",
    "        # For any remaining NaN, use original simple imputation\n",
    "        imputed_series = imputed_series.fillna(method='ffill').fillna(method='bfill').fillna(series.mean())\n",
    "        \n",
    "        return imputed_series\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  STL imputation failed: {str(e)}. Using simple imputation.\")\n",
    "        return temp_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3. GRID SEARCH FOR OPTIMAL PARAMETERS\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMING GRID SEARCH FOR OPTIMAL PARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def hw_grid_search(series, seasonal_periods_list=[7, 30, 365], \n",
    "                  trend_types=['add', 'mul'], seasonal_types=['add', 'mul']):\n",
    "    \"\"\"\n",
    "    Perform grid search to find optimal parameters for Holt-Winters forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas Series\n",
    "        Time series to forecast\n",
    "    seasonal_periods_list : list\n",
    "        List of seasonal periods to try\n",
    "    trend_types : list\n",
    "        List of trend types to try ('add', 'mul')\n",
    "    seasonal_types : list\n",
    "        List of seasonal types to try ('add', 'mul')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with best parameters and model\n",
    "    \"\"\"\n",
    "    best_mse = float('inf')\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    \n",
    "    # Split data for training and testing (use last 60 days for testing)\n",
    "    train_size = len(series) - 60\n",
    "    if train_size <= 0:\n",
    "        train_size = int(len(series) * 0.8)  # Use 80% for training if series is too short\n",
    "    \n",
    "    train_data = series.iloc[:train_size]\n",
    "    test_data = series.iloc[train_size:]\n",
    "    \n",
    "    print(f\"\\nGrid Search for {series.name}\")\n",
    "    \n",
    "    # Grid search\n",
    "    for seasonal_period in seasonal_periods_list:\n",
    "        # Skip if seasonal period is too large for the data\n",
    "        if seasonal_period >= len(train_data) / 2:\n",
    "            print(f\"  Skipping seasonal_period={seasonal_period} (too large for data)\")\n",
    "            continue\n",
    "            \n",
    "        for trend_type in trend_types:\n",
    "            for seasonal_type in seasonal_types:\n",
    "                try:\n",
    "                    # Fit model\n",
    "                    model = ExponentialSmoothing(\n",
    "                        train_data,\n",
    "                        trend=trend_type,\n",
    "                        seasonal=seasonal_type,\n",
    "                        seasonal_periods=seasonal_period,\n",
    "                        use_boxcox=False,\n",
    "                        initialization_method=\"estimated\"\n",
    "                    ).fit(optimized=True, remove_bias=True)\n",
    "                    \n",
    "                    # Forecast and calculate error\n",
    "                    forecast = model.forecast(len(test_data))\n",
    "                    mse = mean_squared_error(test_data, forecast)\n",
    "                    \n",
    "                    print(f\"  Period={seasonal_period}, Trend={trend_type}, Seasonal={seasonal_type}, MSE={mse:.4f}\")\n",
    "                    \n",
    "                    # Update best parameters if this is better\n",
    "                    if mse < best_mse:\n",
    "                        best_mse = mse\n",
    "                        best_params = {\n",
    "                            'seasonal_period': seasonal_period,\n",
    "                            'trend': trend_type,\n",
    "                            'seasonal': seasonal_type,\n",
    "                            'mse': mse\n",
    "                        }\n",
    "                        best_model = model\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error with period={seasonal_period}, trend={trend_type}, seasonal={seasonal_type}: {e}\")\n",
    "    \n",
    "    if best_model is None:\n",
    "        print(f\"No valid model found. Using default parameters.\")\n",
    "        best_params = {\n",
    "            'seasonal_period': seasonal_periods[series.name] if series.name in seasonal_periods else 7,\n",
    "            'trend': 'add',\n",
    "            'seasonal': 'mul',\n",
    "            'mse': float('inf')\n",
    "        }\n",
    "        \n",
    "    print(f\"\\nBest parameters for {series.name}:\")\n",
    "    print(f\"  Seasonal Period: {best_params['seasonal_period']}\")\n",
    "    print(f\"  Trend Type: {best_params['trend']}\")\n",
    "    print(f\"  Seasonal Type: {best_params['seasonal']}\")\n",
    "    print(f\"  MSE: {best_params['mse']:.4f}\")\n",
    "    \n",
    "    return {'params': best_params, 'model': best_model}\n",
    "\n",
    "# Specify potential parameters for grid search\n",
    "# For computational efficiency, we'll limit the options\n",
    "seasonal_periods_options = {\n",
    "    'RR': [365, 183, 90],    # Annual, semi-annual, quarterly\n",
    "    'TAVG': [365, 30, 15],   # Annual, monthly, half-monthly\n",
    "    'RH_AVG': [30, 14, 7]    # Monthly, bi-weekly, weekly\n",
    "}\n",
    "\n",
    "# Results container\n",
    "grid_search_results = {}\n",
    "\n",
    "# Perform grid search for each variable\n",
    "for var in target_vars:\n",
    "    print(f\"\\nPerforming grid search for {var}...\")\n",
    "    grid_search_results[var] = hw_grid_search(\n",
    "        bmkg_filled[var], \n",
    "        seasonal_periods_list=seasonal_periods_options[var],\n",
    "        trend_types=['add'],  # Limit to additive trend for efficiency\n",
    "        seasonal_types=['mul']  # Multiplicative seasonality based on domain knowledge\n",
    "    )\n",
    "\n",
    "# Extract optimal parameters\n",
    "optimal_params = {var: results['params'] for var, results in grid_search_results.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 4. FORECASTING USING OPTIMAL PARAMETERS\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING FORECASTS WITH OPTIMAL PARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Forecast horizon (120 days)\n",
    "forecast_horizon = 120\n",
    "\n",
    "# Create forecast models using optimal parameters\n",
    "forecast_models = {}\n",
    "forecasts = {}\n",
    "\n",
    "for var in target_vars:\n",
    "    print(f\"\\nFitting {var} model with optimal parameters...\")\n",
    "    \n",
    "    best_period = optimal_params[var]['seasonal_period']\n",
    "    best_trend = optimal_params[var]['trend']\n",
    "    best_seasonal = optimal_params[var]['seasonal']\n",
    "    \n",
    "    try:\n",
    "        # Create model with optimal parameters\n",
    "        model = ExponentialSmoothing(\n",
    "            bmkg_filled[var],\n",
    "            trend=best_trend,\n",
    "            seasonal=best_seasonal,\n",
    "            seasonal_periods=best_period,\n",
    "            initialization_method=\"estimated\"\n",
    "        ).fit(optimized=True, remove_bias=True)\n",
    "        \n",
    "        # Store model and generate forecast\n",
    "        forecast_models[var] = model\n",
    "        forecasts[var] = model.forecast(forecast_horizon)\n",
    "        \n",
    "        print(f\"Model parameters:\")\n",
    "        print(f\"  Alpha (level): {model.params['smoothing_level']:.4f}\")\n",
    "        print(f\"  Beta (trend): {model.params['smoothing_trend']:.4f}\")\n",
    "        print(f\"  Gamma (seasonal): {model.params['smoothing_seasonal']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model for {var}: {e}\")\n",
    "        print(\"Using simpler model as fallback...\")\n",
    "        \n",
    "        # Fallback to simpler model if optimal one fails\n",
    "        model = ExponentialSmoothing(\n",
    "            bmkg_filled[var],\n",
    "            trend='add',\n",
    "            seasonal='mul',\n",
    "            seasonal_periods=seasonal_periods[var],\n",
    "            initialization_method=\"estimated\"\n",
    "        ).fit(optimized=True, remove_bias=True)\n",
    "        \n",
    "        forecast_models[var] = model\n",
    "        forecasts[var] = model.forecast(forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# 5. VISUALIZING FORECASTS\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VISUALIZING FORECASTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create forecast index (dates)\n",
    "last_date = bmkg_filled.index[-1]\n",
    "forecast_index = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon, freq='D')\n",
    "\n",
    "# Visualize forecasts for each variable\n",
    "for var in target_vars:\n",
    "    # Historical data (last 365 days for context)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    recent_data = bmkg_filled[var].iloc[-365:]\n",
    "    \n",
    "    # Plot historical data\n",
    "    plt.plot(recent_data.index, recent_data, label='Historical Data', color='blue', alpha=0.6)\n",
    "    \n",
    "    # Plot forecast\n",
    "    plt.plot(forecast_index, forecasts[var], label='Forecast', color='red', linewidth=2)\n",
    "    \n",
    "    # Calculate confidence intervals (using in-sample RMSE)\n",
    "    in_sample_rmse = np.sqrt(mean_squared_error(\n",
    "        bmkg_filled[var][:-forecast_horizon], \n",
    "        forecast_models[var].fittedvalues[:-forecast_horizon]\n",
    "    ))\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    plt.fill_between(\n",
    "        forecast_index,\n",
    "        forecasts[var] - 1.96 * in_sample_rmse,\n",
    "        forecasts[var] + 1.96 * in_sample_rmse,\n",
    "        color='red', alpha=0.2, label='95% Confidence Interval'\n",
    "    )\n",
    "    \n",
    "    # Add threshold reference lines based on agricultural criteria\n",
    "    if var == 'RR':\n",
    "        plt.axhline(y=2, color='orange', linestyle='--', alpha=0.7, label='Lower Threshold (2 mm)')\n",
    "        plt.axhline(y=15, color='red', linestyle='--', alpha=0.7, label='Upper Threshold (15 mm)')\n",
    "    elif var == 'TAVG':\n",
    "        plt.axhline(y=20, color='orange', linestyle='--', alpha=0.7, label='Lower Threshold (20°C)')\n",
    "        plt.axhline(y=35, color='red', linestyle='--', alpha=0.7, label='Upper Threshold (35°C)')\n",
    "    elif var == 'RH_AVG':\n",
    "        plt.axhline(y=60, color='orange', linestyle='--', alpha=0.7, label='Lower Threshold (60%)')\n",
    "        plt.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='Upper Threshold (90%)')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.title(f'{var} - Forecast for {forecast_horizon} Days')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    save_plt(f'forecast_{var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 6. CLASSIFICATION AND DECISION SYSTEM\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SETTING UP CLASSIFICATION AND DECISION SYSTEM\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create DataFrame to store forecasts\n",
    "forecast_df = pd.DataFrame(index=forecast_index)\n",
    "for var in target_vars:\n",
    "    forecast_df[var] = forecasts[var]\n",
    "\n",
    "# Classification functions based on agricultural thresholds\n",
    "def classify_rr(value):\n",
    "    \"\"\"Classify rainfall values into agricultural risk categories.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"Tidak Ada Data\"\n",
    "    elif value < 2:\n",
    "        return \"Kering (Risiko)\"\n",
    "    elif 2 <= value <= 15:\n",
    "        return \"Optimal\"\n",
    "    else:\n",
    "        return \"Banjir (Risiko)\"\n",
    "\n",
    "def classify_tavg(value):\n",
    "    \"\"\"Classify temperature values into agricultural risk categories.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"Tidak Ada Data\"\n",
    "    elif value < 20:\n",
    "        return \"Dingin (Risiko)\"\n",
    "    elif 20 <= value <= 35:\n",
    "        return \"Optimal\"\n",
    "    else:\n",
    "        return \"Panas (Risiko)\"\n",
    "\n",
    "def classify_rh(value):\n",
    "    \"\"\"Classify humidity values into agricultural risk categories.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"Tidak Ada Data\"\n",
    "    elif value < 60:\n",
    "        return \"Kering (Risiko)\"\n",
    "    elif 60 <= value <= 90:\n",
    "        return \"Optimal\"\n",
    "    else:\n",
    "        return \"Lembab Ekstrem (Risiko)\"\n",
    "\n",
    "# Apply classifications to forecast data\n",
    "forecast_df['RR_Status'] = forecast_df['RR'].apply(classify_rr)\n",
    "forecast_df['TAVG_Status'] = forecast_df['TAVG'].apply(classify_tavg)\n",
    "forecast_df['RH_AVG_Status'] = forecast_df['RH_AVG'].apply(classify_rh)\n",
    "\n",
    "# Combine status into a single category column\n",
    "forecast_df['Kategori'] = forecast_df['RR_Status'] + ' / ' + forecast_df['TAVG_Status'] + ' / ' + forecast_df['RH_AVG_Status']\n",
    "\n",
    "# Calculate weighted score\n",
    "def calculate_score(row):\n",
    "    \"\"\"Calculate weighted score based on variable classifications.\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Check if we have valid data\n",
    "    if pd.isna(row['RR']) or pd.isna(row['TAVG']) or pd.isna(row['RH_AVG']):\n",
    "        return 0\n",
    "        \n",
    "    # Assign weights: RR (40%), TAVG (40%), RH_AVG (20%)\n",
    "    if 'Optimal' in row['RR_Status']:\n",
    "        score += 40\n",
    "    if 'Optimal' in row['TAVG_Status']:\n",
    "        score += 40\n",
    "    if 'Optimal' in row['RH_AVG_Status']:\n",
    "        score += 20\n",
    "        \n",
    "    return score\n",
    "\n",
    "# Apply scoring function\n",
    "forecast_df['Skor'] = forecast_df.apply(calculate_score, axis=1)\n",
    "\n",
    "# Decision logic\n",
    "def make_decision(row):\n",
    "    \"\"\"Determine planting recommendation based on scores and status.\"\"\"\n",
    "    # Handle missing data\n",
    "    if pd.isna(row['RR']) or pd.isna(row['TAVG']) or pd.isna(row['RH_AVG']):\n",
    "        return \"Bera\"\n",
    "        \n",
    "    # Prioritize extreme conditions\n",
    "    if 'Banjir' in row['RR_Status'] or 'Panas' in row['TAVG_Status']:\n",
    "        return \"Bera (Risiko Tinggi)\"\n",
    "        \n",
    "    # Score-based decisions\n",
    "    if row['Skor'] >= 70:\n",
    "        return \"Tanam\"\n",
    "    elif 50 <= row['Skor'] < 70:\n",
    "        return \"Tanam (Waspada)\"\n",
    "    else:\n",
    "        return \"Bera\"\n",
    "\n",
    "# Apply decision function\n",
    "forecast_df['Keputusan'] = forecast_df.apply(make_decision, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 7. HARVEST CALENDAR INTEGRATION\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INTEGRATING WITH HARVEST CALENDAR\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set planting date for simulation\n",
    "planting_date = pd.Timestamp('2025-01-11')\n",
    "print(f\"Simulating planting on: {planting_date}\")\n",
    "\n",
    "# Create rice growth window (typically ~100 days from planting to harvest)\n",
    "growth_duration = 100  # days\n",
    "harvest_date = planting_date + pd.Timedelta(days=growth_duration)\n",
    "print(f\"Expected harvest date: {harvest_date}\")\n",
    "\n",
    "# Check if harvest date is within our forecast window\n",
    "if harvest_date in forecast_df.index:\n",
    "    # Check rainfall conditions 7 days before harvest\n",
    "    pre_harvest_window = pd.date_range(end=harvest_date, periods=7, freq='D')\n",
    "    pre_harvest_rain = forecast_df.loc[forecast_df.index.isin(pre_harvest_window), 'RR']\n",
    "    \n",
    "    # Determine if there's a rainfall risk (>10 mm/day)\n",
    "    harvest_risk = (pre_harvest_rain > 10).any()\n",
    "    \n",
    "    if harvest_risk:\n",
    "        harvest_recommendation = \"Percepat Panen (Risiko Hujan)\"\n",
    "        print(f\"Recommendation: {harvest_recommendation}\")\n",
    "    else:\n",
    "        harvest_recommendation = \"Panen Sesuai Jadwal\"\n",
    "        print(f\"Recommendation: {harvest_recommendation}\")\n",
    "        \n",
    "    # Add harvest recommendation to forecast data\n",
    "    for date in pre_harvest_window:\n",
    "        if date in forecast_df.index:\n",
    "            forecast_df.loc[date, 'Keputusan'] = harvest_recommendation\n",
    "else:\n",
    "    print(\"Harvest date is outside the forecast window.\")\n",
    "\n",
    "# ==============================\n",
    "# 8. FORECAST EVALUATION AND VALIDATION\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATING FORECAST PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Function to calculate forecast metrics\n",
    "def calculate_forecast_metrics(actual, forecast):\n",
    "    \"\"\"Calculate common forecast accuracy metrics.\"\"\"\n",
    "    # Handle NaN values\n",
    "    valid = ~np.isnan(actual) & ~np.isnan(forecast)\n",
    "    if sum(valid) == 0:\n",
    "        return {'mse': np.nan, 'rmse': np.nan, 'mae': np.nan, 'mape': np.nan, 'r2': np.nan}\n",
    "    \n",
    "    actual = actual[valid]\n",
    "    forecast = forecast[valid]\n",
    "    \n",
    "    # Avoid division by zero in MAPE\n",
    "    mape_valid = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mape_valid] - forecast[mape_valid]) / actual[mape_valid])) * 100 if sum(mape_valid) > 0 else np.nan\n",
    "    \n",
    "    return {\n",
    "        'mse': mean_squared_error(actual, forecast),\n",
    "        'rmse': np.sqrt(mean_squared_error(actual, forecast)),\n",
    "        'mae': mean_absolute_error(actual, forecast),\n",
    "        'mape': mape,\n",
    "        'r2': r2_score(actual, forecast)\n",
    "    }\n",
    "\n",
    "# Calculate in-sample metrics for each variable\n",
    "metrics = {}\n",
    "for var in target_vars:\n",
    "    actual = bmkg_filled[var][forecast_models[var].fittedvalues.index]\n",
    "    predicted = forecast_models[var].fittedvalues\n",
    "    metrics[var] = calculate_forecast_metrics(actual, predicted)\n",
    "    \n",
    "    print(f\"\\nPerformance metrics for {var}:\")\n",
    "    print(f\"  MSE: {metrics[var]['mse']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics[var]['rmse']:.4f}\")\n",
    "    print(f\"  MAE: {metrics[var]['mae']:.4f}\")\n",
    "    print(f\"  MAPE: {metrics[var]['mape']:.2f}%\")\n",
    "    print(f\"  R-squared: {metrics[var]['r2']:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 9. SAVE RESULTS\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare final forecast dataframe for output\n",
    "output_df = forecast_df.reset_index().rename(columns={'index': 'Tanggal'})\n",
    "\n",
    "# Select and reorder columns for final output\n",
    "final_columns = ['Tanggal', 'RR', 'TAVG', 'RH_AVG', 'Skor', 'Kategori', 'Keputusan']\n",
    "output_df = output_df[final_columns]\n",
    "\n",
    "# Save to CSV\n",
    "output_path = f\"{csv_output_dir}/rice_planting_forecast_{pd.Timestamp.now().strftime('%Y%m%d')}.csv\"\n",
    "output_df.to_csv(output_path, index=False)\n",
    "print(f\"Forecast saved to: {output_path}\")\n",
    "\n",
    "# Create summary visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Stacked bar for decision counts\n",
    "plt.subplot(2, 1, 1)\n",
    "decision_counts = output_df['Keputusan'].value_counts()\n",
    "decision_counts.plot(kind='bar', color=['green', 'orange', 'red', 'gray'])\n",
    "plt.title('Distribution of Planting Decisions')\n",
    "plt.xlabel('Decision')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Timeline of decisions\n",
    "plt.subplot(2, 1, 2)\n",
    "# Create numeric mapping for decisions for colormap\n",
    "decision_map = {\n",
    "    'Tanam': 3,\n",
    "    'Tanam (Waspada)': 2,\n",
    "    'Bera': 1,\n",
    "    'Bera (Risiko Tinggi)': 0,\n",
    "    'Percepat Panen (Risiko Hujan)': 4\n",
    "}\n",
    "output_df['Decision_Code'] = output_df['Keputusan'].map(decision_map)\n",
    "\n",
    "# Plot decision timeline\n",
    "plt.scatter(output_df['Tanggal'], output_df['Decision_Code'], c=output_df['Decision_Code'], \n",
    "            cmap='RdYlGn', alpha=0.8, s=50)\n",
    "plt.yticks([0, 1, 2, 3, 4], \n",
    "           ['Bera (Risiko Tinggi)', 'Bera', 'Tanam (Waspada)', 'Tanam', 'Percepat Panen'])\n",
    "plt.title('Timeline of Planting Decisions')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "save_plt('decision_summary')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Results saved to {output_path}\")\n",
    "print(f\"Plots saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tugas-akhir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
